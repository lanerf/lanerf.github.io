<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="LANe : Lighting Aware Neural Fields for Compositional Scene Synthesis">
  <meta name="keywords" content="LANe, NeRF, Automated Driving">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>LANe : Lighting Aware Neural Fields for Compositional Scene Synthesis</title>
  
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel-img.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/bulma-carousel-img.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>



<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h4 class="title is-1 publication-title" style="margin-bottom:0;">LANe : Lighting-Aware Neural Fields for Compositional Scene Synthesis</h4>
        
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">

                <a href=""
                   class="external-link button is-normal is-rounded is-dark" disabled>
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark" disabled>
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            
              <!-- Video Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark" disabled>
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link.  -->
              <span class="link-block">
                  <a href=""
                   class="external-link button is-normal is-rounded is-dark" disabled >
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>

              <!-- Dataset Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark" disabled>
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



 <!-- Teaser Figure -->.  
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      
      <div class="column">
        <div class="content">
          <h2 class="title is-4 has-text-centered"> </h2>
            <img src="./static/images/teaser_lane.png" alt="DIV2K Desert" height="100%" width="100%"> 
            <div class="content has-text-justified">
              <p>
                We present <b>Lighting-Aware Neural Fields (LANe) for compositional scene synthesis</b>. With the disentanglement of the
                learnt world model and a class specific object model, our approach is capable of arbitrarily composing objects into the
                 scene. Our novel light field modulated representation allows the object model to be rendered in scenes in a 
                 lighting-aware manner. 
              </p>
              <p>
                The figure above shows the same world model learnt and reused as background scenes on each row, 
                and object models composed into scene in arbitrary poses and locations under different lighting conditions. 
                Note that the newly synthesized objects are shaded appropriately based on the scene lighting condition in which it is 
                placed, which indicates our approach LANe can compose multiple object models and world model with physical realistic 
                and consistent results.
              </p>
            </div>
        </div>        
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Neural fields have recently enjoyed great success in representing and rendering 3D  scenes. However, most state-of-the-art implicit 
            representations model static or dynamic scenes as a whole, with minor variations. Existing work on learning disentangled world and 
            object fields do not consider the problem of composing objects into different world fields in a lighting-aware manner. 
          </p>
          <p>
            We present Lighting-Aware Neural Field (LANe) for the compositional synthesis of driving scenes in a physically consistent manner. 
            Specifically, we learn a scene representation that disentangles the static background and transient elements into a world-NeRF and 
            class-specific object-NeRFs to allow compositional synthesis of multiple objects in the scene. Furthermore, we explicitly designed 
            both the world and object models to handle lighting variation, which allows us to compose objects into scenes with spatially varying 
            lighting. This is achieved by constructing a light field of the scene and using it in conjunction with a learned shader to modulate
            the appearance of the object NeRFs. We demonstrate the performance of our model on a synthetic dataset of diverse lighting conditions 
            rendered with the CARLA simulator, as well as a novel real-world dataset of cars collected at different times of the day.
          </p>
           <p>
            Our approach composes objects learned from one scene into an entirely different scene whilst still respecting the lighting 
            variations in the novel scene, and show that it outperforms state of the art compositional scene synthesis on the challenging 
            dataset setup.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video --> 
        <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered">Video</h2>
        <div class="publication-video">
          <iframe src="./static/images/lane_video.mp4"
                  frameborder="0" allow="encrypted-media" height="80%" width="80%" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    -->
  </div>
</section>




<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
            <h2 class="title is-3 has-text-centered" center>Method</h2>
            <figure class="image is-inline-block" >
              <img src="./static/images/framework.png" alt="Method">
          </figure>
          <p>
            Overview of the proposed approach. We model the scene with a seperate world-NeRF, which lighting-aware by training on 
            the same scene under different lighting conditions, and a class specific object-NeRF, which  use information from the 
            scene-NeRF to train the object NeRF. 
          </p>
          </div>
    </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
            <h2 class="title is-3 has-text-centered" center>Comparison</h2>
            <figure class="image is-inline-block" >
              <img src="./static/images/spatially_varying_lighting.png" >
          </figure>
          <p>
            Comparison: LANe can synthesize scenes with object models that respect spatially varying lighting. This figure shows 
            the object model moving through the scene with spatially varying lighting, we observe that the object gets brighter as 
            it enters a region of light from a region of shadow.
          </p>
          </div>
    </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
            <h2 class="title is-3 has-text-centered" center>Downstream Task Validation</h2>
        <p>
          The lack of diversity in lighting condition is a known issue with manually curated autonomous driving datasets. For 
          instance, the <a href="https://www.cvlibs.net/datasets/kitti/">KITTI dataset</a> was captured around the noon time, with similar lighting and shadow 
          conditions across different sequences. Previous work such as <a href="https://arxiv.org/abs/2204.00644"> SIMBAR </a> have shown that deep learning models trained with such 
          limited lighting conditions are unable to generalize to the plethora of lighting conditions encountered in the real-world, 
          and validated the effectiveness of scene relighting as a useful data augmentation methodology for vision tasks. Our 
          approach LANe can be leveraged for lighting-aware data manipulation and data augmentation for downstream autonomous 
          driving vision tasks.
        </p>
      </div>
    </div>
</section>


  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <br />
      <p>Page template borrowed from <a href="https://nerfies.github.io/"><span class="dnerf">D-NeRF</span></a>.</p>
    </div>
  </div>
</footer>

</body>
</html>
